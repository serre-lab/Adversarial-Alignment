{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Adversarial Alignment: breaking the trade-off between the strength of an attack and its relevance to human perception   Drew Linsley*1, 2, Pinyuan Feng*3, Thibaut Boissin4, Alekh Karkada Ashok1, Thomas Fel5, Stephanie Olaiya1, Thomas Serre1, 2, 3, 5  Department of Cognitive, Linguistic, &amp; Psychological Sciences, Brown University, Providence, RI, USA   Carney Institute for Brain Science, Brown University, Providence, RI, USA   Department of Computer Science, Brown University, Providence, RI, USA   Institut de Recherche Technologique Saint-Exup'ery, Toulouse, France    Artificial and Natural Intelligence Toulouse Institute (ANITI), Toulouse, France  <p> Read our paper \u00bb View our GitHub \u00bb Results   \u00b7   Model Info   \u00b7   Harmonization   \u00b7   ClickMe   \u00b7   Serre Lab @ Brown </p>"},{"location":"#abstract","title":"Abstract","text":"<p>Deep neural networks (DNNs) are known to have a fundamental sensitivity to adversarial attacks, perturbations of the input that are imperceptible to humans yet powerful enough to change the visual decision of a model. Adversarial attacks have long been considered the \"Achilles' heel\" of deep learning, which may eventually force a shift in modeling paradigms. Nevertheless, the formidable capabilities of modern large-scale DNNs have somewhat eclipsed these early concerns. Do adversarial attacks continue to pose a threat to DNNs?</p> <p>In this study, we investigate how the robustness of DNNs to adversarial attacks has evolved as their accuracy on ImageNet has continued to improve. We measure adversarial robustness in two different ways: First, we measure the smallest adversarial attack needed to cause a model to change its object categorization decision. Second, we measure how aligned successful attacks are with the features that humans find diagnostic for object recognition. We find that adversarial attacks are inducing bigger and more easily detectable changes to image pixels as DNNs grow better on ImageNet, but these attacks are also becoming less aligned with the features that humans find diagnostic for object recognition. To better understand the source of this trade-off and if it is a byproduct of DNN architectures or the routines used to train them, we turn to the neural harmonizer, a DNN training routine that encourages models to leverage the same features humans do to solve tasks. Harmonized DNNs achieve the best of both worlds and experience attacks that are both detectable and affect object features that humans find diagnostic for recognition, meaning that attacks on these models are more likely to be rendered ineffective by inducing similar effects on human perception. Our findings suggest that the sensitivity of DNNs to adversarial attacks can be mitigated by DNN scale, data scale, and training routines that align models with biological intelligence. We release our code and data to support this goal.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use or build on our work as part of your workflow in a scientific publication, please consider citing the official paper:</p> <pre><code>@article{linsley2023adversarial,\n      title={Adversarial alignment: Breaking the trade-off between the strength of an attack and its relevance to human perception}, \n      author={Drew Linsley and Pinyuan Feng and Thibaut Boissin and Alekh Karkada Ashok and Thomas Fel and Stephanie Olaiya and Thomas Serre},\n      year={2023},\n      eprint={2306.03229},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre> <p>If you have any questions about the paper, please contact Drew at drew_linsley@brown.edu.</p>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>This paper relies heavily on previous work from Serre Lab, notably Harmonization and ClickMe.</p> <pre><code>@article{fel2022aligning,\n  title={Harmonizing the object recognition strategies of deep neural networks with humans},\n  author={Fel, Thomas and Felipe, Ivan and Linsley, Drew and Serre, Thomas},\n  journal={Advances in Neural Information Processing Systems (NeurIPS)},\n  year={2022}\n}\n\n@article{linsley2018learning,\n  title={Learning what and where to attend},\n  author={Linsley, Drew and Shiebler, Dan and Eberhardt, Sven and Serre, Thomas},\n  journal={International Conference on Learning Representations (ICLR)},\n  year={2019}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>The code is released under  MIT license.</p>"},{"location":"models/","title":"Model Information","text":"<ul> <li>In our experiment, 283 models have been tested and the Top-1 ImageNet accuracy for each model refers to Hugging Face results.<ul> <li>\u00b7 125 PyTorch CNN models from timm library</li> <li>\u00b7 121 PyTorch ViT models from timm library</li> <li>\u00b7 15 PyTorch ViT/CNN hybrid architectures from timm library</li> <li>\u00b7 14 Tensorflow Harmonized models from harmonizatin library</li> <li>\u00b7 4 Baseline models</li> <li>\u00b7 4 models that were trained for robustness to adversarial example</li> </ul> </li> </ul> Architecture Model Versions CNN VGG 8 CNN ResNet 8 CNN EfficientNet 7 CNN ConvNext 6 CNN MobileNet 10 CNN Inception 3 CNN DenseNet 4 CNN RegNet 22 CNN Xception 4 CNN MixNet 4 CNN DPN 6 CNN DarkNet 1 CNN NFNet 11 CNN TinyNet 5 CNN LCNet 3 CNN DLA 12 CNN MnasNet 4 CNN CSPNet 3 ViT General ViT 8 ViT MobileViT 10 ViT Swin 22 ViT MaxViT 14 ViT DeiT 24 ViT CaiT 10 ViT XCiT 28 ViT EVA 5 Hybrid VOLO 8 Hybrid CoAtNet 13"},{"location":"results/","title":"Results","text":""},{"location":"results/#perturbation-torlerance","title":"Perturbation Torlerance","text":"<p>The perturbation tolerance of DNNs has significantly increased as they have improved on ImageNet. Each dot denotes a DNN's ImageNet accuracy vs. its average \\ell_2 robustness radius to \\ell_2 PGD attacks, which we call ''perturbation tolerance''. Arrows show the change of a DNN in both dimensions after it has been trained with the neural harmonizer. There is a significant positive correlation between ImageNet accuracy and perturbation tolerance (\\rho_{s} = 0.70, p &lt; 0.001). Error bars denote standard error, and variance may be so small for some models that they are not visible.</p>"},{"location":"results/#adversarial-alignment","title":"Adversarial Alignment","text":"<p>Successful adversarial attacks on DNNs are becoming less aligned with human perception as they have improved on ImageNet. Each dot denotes a DNN's ImageNet accuracy vs. the average Spearman correlation between successful attacks an images' human feature importance maps from ClickMe. We call this correlation a DNN's adversarial alignment. Arrows show the change of a DNN in both dimensions after it has been trained with the neural harmonizer. Error bars denote standard error, and variance may be so small for some models that they are not visible.</p>"},{"location":"results/#qualitative-results","title":"Qualitative Results","text":"<p>\\ell_2 PGD adversarial attacks for DNNs. Plotted here are ImageNet images, human feature importance maps from ClickMe, and adversarial attacks for a variety of DNNs. Attacked images are included for the image of a monkey at the top (zoom in to see attack details). The red box shows inanimate categories, and the blue box shows animate categories.</p>"}]}