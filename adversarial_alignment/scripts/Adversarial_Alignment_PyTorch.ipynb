{"cells":[{"cell_type":"markdown","metadata":{"id":"rGBrVRVw7wjy"},"source":["## Load Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30375,"status":"ok","timestamp":1684301130933,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"Dfeo4w-HnmmT","outputId":"4d4341b0-79b8-4730-c112-edceda83425d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","cuda\n"]}],"source":["import csv\n","import pandas as pd\n","import os\n","import tensorflow as tf\n","import time\n","import gc\n","import numpy as np\n","from scipy.stats import spearmanr\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import transforms\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import IPython\n","from google.colab import output\n","from IPython.display import clear_output\n"]},{"cell_type":"markdown","metadata":{"id":"HQjinWJb7-Uy"},"source":["## Load timm model"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5413,"status":"ok","timestamp":1684301136342,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"P9n1KCQAlql6","outputId":"ef32a895-749b-4a85-9ee3-bd68075371c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Done.\n"]}],"source":["!pip install timm==0.8.10.dev0\n","clear_output()\n","print(\"Done.\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8711,"status":"ok","timestamp":1684301145048,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"drSpUluCsl_E","outputId":"0edb25eb-d43e-4293-8950-647703141bd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["vgg11 Done.\n"]}],"source":["import timm\n","model_name = 'vgg11'\n","model = timm.create_model(model_name, num_classes=1000, pretrained=True).to(device)\n","model.eval()\n","\n","clear_output()\n","print(model_name, \"Done.\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684301145048,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"I0TKhZ5FnCNA","outputId":"09b1ef73-03c9-4761-a820-b049c576fe10"},"outputs":[{"name":"stdout","output_type":"stream","text":["Compose(\n","    Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n","    CenterCrop(size=(224, 224))\n","    ToTensor()\n","    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",")\n"]}],"source":["data_config = timm.data.resolve_model_data_config(model)\n","timm_transforms = timm.data.create_transform(**data_config, is_training=False)\n","print(timm_transforms)"]},{"cell_type":"markdown","metadata":{"id":"yme0A21p8G-J"},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":10097,"status":"ok","timestamp":1684301155141,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"AFLwvbEhnpe4"},"outputs":[],"source":["!cp '/content/drive/MyDrive/Research/Adversarial_Attacks_Model-VS-Human/datasets/clickme_test_1000.tfrecords' ./"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1684301155142,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"Xzptjx05oICa"},"outputs":[],"source":["class Dataset:\n","    def __init__(self, data_path):\n","        self.data_path = data_path\n","        \n","        self.AUTO = tf.data.AUTOTUNE\n","\n","        self._feature_description = {\n","            \"image\"       : tf.io.FixedLenFeature([], tf.string, default_value=''),\n","            \"heatmap\"     : tf.io.FixedLenFeature([], tf.string, default_value=''),\n","            \"label\"       : tf.io.FixedLenFeature([], tf.int64, default_value=0),\n","        }\n","\n","    def parse_prototype(self, prototype, training=False):\n","        data    = tf.io.parse_single_example(prototype, self._feature_description)\n","\n","        image   = tf.io.decode_raw(data['image'], tf.float32)\n","        image   = tf.reshape(image, (224, 224, 3))\n","        image   = tf.cast(image, tf.float32)\n","\n","        heatmap = tf.io.decode_raw(data['heatmap'], tf.float32)\n","        heatmap = tf.reshape(heatmap, (224, 224, 1))\n","\n","        label   = tf.cast(data['label'], tf.int32)\n","        label   = tf.one_hot(label, 1_000)\n","\n","        return image, heatmap, label\n","\n","    def get_dataset(self, batch_size, training=False):\n","        deterministic_order = tf.data.Options()\n","        deterministic_order.experimental_deterministic = True\n","\n","        dataset = tf.data.TFRecordDataset([self.data_path], num_parallel_reads=self.AUTO)\n","        dataset = dataset.with_options(deterministic_order) \n","        \n","        dataset = dataset.map(self.parse_prototype, num_parallel_calls=self.AUTO)\n","        \n","        dataset = dataset.batch(batch_size, drop_remainder=True)\n","        dataset = dataset.prefetch(self.AUTO)\n","\n","        return dataset\n","        \n","datapath = \"/content/clickme_test_1000.tfrecords\"\n","data = Dataset(datapath)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1684301155142,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"U6B7_3SS1Tsw"},"outputs":[],"source":["from scipy.ndimage import gaussian_filter\n","import matplotlib.pyplot as plt\n","def show(img, p=False, smooth=False, **kwargs):\n","    \"\"\" Display torch/tf tensor \"\"\" \n","    try:\n","        img = img.detach().cpu()\n","    except:\n","        img = np.array(img)\n","\n","    img = np.array(img, dtype=np.float32)\n","\n","    # check if channel first\n","    if img.shape[0] == 1:\n","        img = img[0]\n","    elif img.shape[0] == 3:\n","        img = np.moveaxis(img, 0, -1)\n","\n","    # check if cmap\n","    if img.shape[-1] == 1:\n","        img = img[:,:,0]\n","\n","    # normalize\n","    if img.max() > 1 or img.min() < 0:\n","        img -= img.min(); img/=img.max()\n","\n","    # check if clip percentile\n","    if p is not False:\n","        img = np.clip(img, np.percentile(img, p), np.percentile(img, 100-p))\n","\n","    if smooth and len(img.shape) == 2:\n","        img = gaussian_filter(img, smooth)\n","\n","    plt.imshow(img, **kwargs)\n","    plt.axis('off')\n","    plt.grid(None)"]},{"cell_type":"markdown","metadata":{"id":"W_hwrYbY8MVc"},"source":["## Attack"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1684301155143,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"p0_if3nyoH97"},"outputs":[],"source":["loss = nn.CrossEntropyLoss()\n","\n","class Attack:\n","    def __init__(self):\n","        pass\n","    \n","    def l2(self, x):\n","        # return torch.sqrt(torch.sum(torch.square(x), (1,2,3))) \n","        batch_size = x.shape[0]\n","        grad_norms = torch.norm(x.reshape(batch_size, -1), p=2, dim=1)\n","        return grad_norms.reshape(batch_size, 1, 1, 1)\n","\n","    def l2_pgd_attack(self, model, images, labels, eps, alpha=10/255, iters=5):   \n","        x0 = images.clone().detach()\n","        xt = x0.clone().detach() \n","        \n","        for i in range(iters):        \n","            xt.requires_grad = True\n","            outputs = model(xt)\n","            \n","            # getting the gradient grad(loss)\n","            model.zero_grad()\n","            cost = loss(outputs, labels)\n","            cost.backward()\n","\n","            # apply the gradient to our current point\n","            grads = xt.grad\n","            x_next = xt.detach() + grads / self.l2(grads) * alpha\n","            \n","            # project the current point on the l2(x0, epsilon) ball :)\n","            delta = x_next - x0\n","            sigma = self.l2(delta)\n","            x_next = x0 + (delta / sigma) * eps\n","            \n","            # ready for the next step\n","            # xt = x_next.detach()\n","            xt = x_next.clamp(min=0, max=1).detach()\n","\n","        return xt"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1684301155143,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"e-mKCYJyoIAS"},"outputs":[],"source":["def execute_attack(model, img, target, eps, alpha=0.5, iters=3):\n","    # Setup an attack\n","    attack_obj = Attack()\n","    perturbed_img = attack_obj.l2_pgd_attack(model, img, target, eps=eps, alpha=0.5, iters=3)\n","\n","    # Re-classify the perturbed image\n","    with torch.no_grad():\n","        output = model(perturbed_img)\n","        pred = torch.argmax(output, axis=-1) # get the index of the max log-probability\n","\n","    del attack_obj\n","\n","    return perturbed_img, pred\n","\n","def write_csv_all(record, path):\n","    header = ['model', 'img', 'label', 'pred', 'eps', 'l2', 'linf', 'spearman']\n","    file_exists = os.path.isfile(path)\n","\n","    with open(path, mode='a+', newline='') as csv_file:\n","        writer = csv.writer(csv_file)\n","        if not file_exists:\n","            writer.writerow(header)\n","        writer.writerow(record)\n","\n","def write_csv_avg(record, path):\n","    header = ['model', 'num_correct', \n","              'avg_eps', 'std_eps', \n","              'avg_l2', 'std_l2', \n","              'avg_linf', 'std_linf', \n","              'avg_spearman', 'std_spearman']\n","    file_exists = os.path.isfile(path)\n","\n","    with open(path, mode='a+', newline='') as csv_file:\n","        writer = csv.writer(csv_file)\n","        if not file_exists:\n","            writer.writerow(header)\n","        writer.writerow(record)\n","\n","from PIL import Image\n","def save_img(image_tensor, path):\n","\n","    # convert the tensor to a PIL image\n","    pil_image = transforms.ToPILImage()(image_tensor.squeeze())\n","\n","    # save the image to disk in PNG format\n","    pil_image.save(path)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1684301155143,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"Pv-fZeryqkQT"},"outputs":[],"source":["from scipy.stats import spearmanr\n","\n","# Computes the Spearman correlation between two sets of heatmaps.\n","def spearman_correlation(a, b):\n","    assert a.shape == b.shape, \"The two sets of images must\" \\\n","                                                 \"have the same shape.\"\n","    assert len(a.shape) == 4, \"The two sets of heatmaps must have shape (1, 1, W, H).\"\n","\n","    rho, _ = spearmanr(a.flatten(), b.flatten())\n","    return rho\n","\n","# Transform tensorflow tensor to pytorch tensor\n","def tf2torch(t): # a batch of image tensors (N, H, W, 3)\n","    t = tf.cast(t, tf.float32).numpy()\n","    if t.shape[-1] in [1, 3]:\n","        t = torch.from_numpy(t.transpose(0, 3, 1, 2)) # torch.from_numpy(np_array.transpose(0, 3, 1, 2)) \n","        return t\n","    return torch.from_numpy(t) # (N, 3, H, W)\n","\n","# Transform tensorflow tensor to numpy array\n","def tf2np(t): # a batch of image tensors (N, H, W, 3)\n","    t = tf.cast(t, tf.float32).numpy()\n","    if t.shape[-1] in [1, 3]:\n","        t = t.transpose(0, 3, 1, 2) # torch.from_numpy(np_array.transpose(0, 3, 1, 2)) \n","        return t\n","    return t # (N, 3, H, W)\n","\n","# Image normalization\n","def img_normalize(imgs):\n","    imgs = imgs - imgs.min()\n","    imgs = imgs / imgs.max()\n","    return imgs\n","\n","# Compute L-infinity norm\n","def linf_loss(x, y):\n","    return np.linalg.norm(x.flatten() - y.flatten(), ord=np.inf)\n","\n","# Compute L-2 norm\n","def l2_loss(x, y):\n","    return np.linalg.norm(x.flatten() - y.flatten(), ord=2)"]},{"cell_type":"markdown","metadata":{"id":"g5sLZF-V8eqx"},"source":["## Main"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":862562,"status":"ok","timestamp":1684302017690,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"uO8ZXXqmGo9b","outputId":"29d20d88-e94a-4c68-f874-8a6a68d22634"},"outputs":[{"name":"stdout","output_type":"stream","text":["Created folder: /content/drive/MyDrive/Research/Adversarial_Attacks_Model-VS-Human/images/vgg11\n","Searching optimal epsilon for image: 1000 | 1000 vgg11 \n"]}],"source":["# Define paths\n","case = \"L2PGD_0.5_3\"\n","results_path_avg = '/content/drive/MyDrive/Research/Adversarial_Attacks_Model-VS-Human/results/' + case + '.csv'\n","results_path_all = '/content/drive/MyDrive/Research/Adversarial_Attacks_Model-VS-Human/results/' + model_name + '.csv'\n","images_path = '/content/drive/MyDrive/Research/Adversarial_Attacks_Model-VS-Human/images/'\n","\n","images_path = os.path.join(images_path, model_name)\n","if not os.path.exists(images_path):\n","    os.makedirs(images_path)\n","    print(f\"Created folder: {images_path}\")\n","else:\n","    print(f\"Folder already exists: {images_path}\")\n","\n","l2_list, linf_list = [], []\n","opt_epsilons = []\n","spearman_scores = []\n","total_cnt, init_correct, aa_correct = 0, 0, 0\n","\n","# Pre-load \n","for imgs, hmps, labels in data.get_dataset(2 , False): # image, heatmap, label\n","    imgs = tf2torch(imgs)\n","    hmps = tf2torch(hmps)\n","\n","    # Apply image transformation to meet the input requirements\n","    nh, nw = timm_transforms.transforms[1].size\n","    _, _, h, w = imgs.shape\n","    if not (h, w) == (nh, nw):\n","        # print(h, w, nh, nw)\n","        transform = transforms.Resize((nh, nw), transforms.InterpolationMode.BICUBIC)\n","        imgs = transform(imgs)\n","        hmps = transform(hmps)\n","\n","    imgs = img_normalize(imgs).to(device)\n","    hmps = img_normalize(hmps)      \n","    labels = tf2torch(labels).to(device)\n","\n","    for img, hmp, label in zip(imgs, hmps, labels):\n","        # print(img.shape, hmp.shape, logit.shape)\n","\n","        # Add a dimension (batch == 1)\n","        img = torch.unsqueeze(img, 0) # (1, 3, H, W)\n","        hmp = torch.unsqueeze(hmp, 0)\n","        label = torch.unsqueeze(label, 0)\n","        target = torch.argmax(label, axis=-1) # tensor([343], device='cuda:0')\n","        # print(img.shape, target.shape)\n","\n","        # Forward pass the data through the model\n","        with torch.no_grad():\n","            output = model(img)\n","            init_pred = torch.argmax(output, axis=-1) # get the index of the max log-probability\n","\n","        # If the initial prediction is wrong, just move on\n","        total_cnt += 1\n","        print(\"\\rSearching optimal epsilon for image: %s | %s %s\" % (\n","            str(total_cnt), \n","            str(1000), model_name), end=\" \")\n","        if init_pred.item() != target.item():\n","            continue\n","        init_correct += 1\n","\n","        # Apply first attack\n","        initial_eps = 10\n","        perturbed_img, perturbed_pred = execute_attack(model=model, img=img, target=target, eps=initial_eps, alpha=0.5)\n","\n","        if perturbed_pred.item() == target.item(): \n","            # Assume 10 is the min eps that fools the model\n","            continue\n","        else:\n","            # key: eps; val: perturbed_img\n","            info = {} # Only store one key-val pair\n","            key = None\n","\n","            initial_eps = 1\n","            perturbed_img, perturbed_pred = execute_attack(model=model, img=img, target=target, eps=initial_eps, alpha=0.5)\n","\n","            key = initial_eps\n","            info[key] = (perturbed_img, perturbed_pred)\n","\n","            if perturbed_pred.item() != target.item():\n","                l, r = 0.001, 1 # search eps between 0.001 and 1\n","                threshold = 0.01\n","            else:\n","                l, r = 1, 10 # search eps between 1 and 10\n","                threshold = 0.1 \n","\n","            # Apply binary search\n","            while r - l >= threshold:\n","                eps = l + (r - l) / 2\n","                perturbed_img, perturbed_pred = execute_attack(model=model, img=img, target=target, eps=eps, alpha=0.5)\n","                if perturbed_pred.item() != target.item():\n","                    r = eps\n","                    if r != key:\n","                        del info[key]\n","                        key = r\n","                        info[key] = (perturbed_img, perturbed_pred)\n","                else:\n","                    l = eps\n","\n","            optimal_eps = r\n","            if r in info:\n","                perturbed_img, perturbed_pred = info[r]\n","            else:\n","                perturbed_img, perturbed_pred = execute_attack(model=model, img=img, target=target, eps=optimal_eps, alpha=0.5)\n","\n","            if not (perturbed_pred.item() != target.item()): \n","                continue\n","\n","        opt_epsilons.append(optimal_eps)\n","\n","        # save images\n","        l, t = str(target.item()), str(perturbed_pred.item())\n","        img_path = os.path.join(images_path, model_name + '_' + l + '_' + t + '.png')\n","        save_img(perturbed_img, img_path)\n","\n","        # Store l2, linf\n","        a, b = perturbed_img.cpu().numpy(), img.cpu().numpy()\n","        l2, linf = l2_loss(a, b), linf_loss(a, b)\n","        l2_list.append(l2)\n","        linf_list.append(linf)\n","\n","        # Spearman correlation\n","        mask = np.abs(np.mean(a - b, axis=1, keepdims=True)) # (1, 1, 224, 224)\n","        spearman_score = spearman_correlation(mask, hmp.numpy())\n","        spearman_scores.append(spearman_score)\n","\n","        # Save the data into \n","        row_data = [\n","            model_name, str(total_cnt-1), str(target.item()), str(perturbed_pred.item()), \n","            str(round(optimal_eps, 6)), str(l2), str(linf), str(spearman_score)\n","        ]\n","\n","        # print(row_data)\n","        # write_csv_all(row_data, results_path_all)\n","        # clear_unused_memo()\n","\n","print(\"\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":484,"status":"ok","timestamp":1684302018172,"user":{"displayName":"FENG Pinyuan","userId":"16238921250041619986"},"user_tz":240},"id":"Fob1Eg-PKbXq","outputId":"1743a35d-9417-4539-e253-ca6550e11dd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["['vgg11', '326', '0.011333984374999998', '0.028316600713116053', '1.3170422', '0.09202564', '0.049354456', '0.012586416', '0.1995636211963146', '0.13514956956593097']\n"]}],"source":["# Save data\n","row_data = [\n","    model_name, str(init_correct), \n","    str(np.mean(opt_epsilons)), str(np.std(opt_epsilons)),\n","    str(np.mean(l2_list)), str(np.std(l2_list)),\n","    str(np.mean(linf_list)), str(np.std(linf_list)),\n","    str(np.mean(spearman_scores)), str(np.std(spearman_scores)),\n","]\n","print(row_data)\n","# write_csv_avg(row_data, results_path_avg)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
